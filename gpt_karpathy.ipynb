{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oEgA5IXeUd-"
      },
      "source": [
        "# Imports and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_94r5PeqeXQB",
        "outputId": "ee2f4f57-0019-4d20-d7c7-ff38e73024b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e5a2af799d0>"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 100\n",
        "eval_interval = 10\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 50\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "# seed\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2njqzOLBIJTF"
      },
      "source": [
        "# Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGChfx-XGdaN",
        "outputId": "f8b2bbe7-a0ee-4028-ad6b-91201115efc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-12-03 08:12:06--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.6’\n",
            "\n",
            "input.txt.6         100%[===================>]   1.06M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-12-03 08:12:06 (16.0 MB/s) - ‘input.txt.6’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download the shakespeare dataset\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA1_j6bUIMEN"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQANfN0NIR9h"
      },
      "source": [
        "### Data Exploration\n",
        "\n",
        "How long is the dataset in characters?\n",
        "What does the text look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGXCSlYBIOKO",
        "outputId": "d17e9921-a170-4c1b-e6d1-f24d272a48c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IBoOTiMIWvy"
      },
      "outputs": [],
      "source": [
        "# print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v9TVu6kILQ3"
      },
      "source": [
        "### Make Vocab\n",
        "\n",
        "Find all unique characters, map them to index and back to character\n",
        "\n",
        "Index can be inputted into model, itos allows decoding model output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8wjrySrIZxI",
        "outputId": "2500f55c-8c63-4c9a-8c17-f24eb87988e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "# sort a list of the unique characters in the text, set takes unique elements on any iterable in this case string\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vN7gvOOIphD",
        "outputId": "109c9f3e-6582-487d-e291-0bd022f56522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9MsmaV-JG6P"
      },
      "source": [
        "### Preprocess Data\n",
        "Just convert everything into indexes and put into tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sKqgDhjI3Va",
        "outputId": "b9bcf43c-6c86-4e61-db42-f9a2c28e5ce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1115394]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "# encode entire dataset (pass everything through stoi), store indexes into a tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "# print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75oMAmUNJETi"
      },
      "source": [
        "### Train Val Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYMWjZOxJBqk"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_l8tQr1JTQA"
      },
      "source": [
        "# Set up Batch function\n",
        "We get random batches of blocks as offsets to starting index using torch.randint\n",
        "\n",
        "The number of batches is our batch dimesnsion (B\n",
        "\n",
        "#### Explaination of individual blocks:\n",
        "For each block we grab (blocksize) number of characters and then we increment by 1 to get the subsequent character after each index of the x array as our labels. We want the model to predict the next character given all previous characters in sequence.\n",
        "\n",
        "Now for each individual block, we provide the context xb[b, :t+1] (all characters prior to the current input and including the current input) and our target the next character y[b,i], b is our batch dimension\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXAqwb1vJZD8"
      },
      "outputs": [],
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "  # generate a small batch of data of inputs x and targets y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLqKo5JprrLl"
      },
      "source": [
        "# Evaluate Loss\n",
        "Since we are sampling batches, we take the average across several sample batches for less fluctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZTXAS6Lrqbe"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "      losses = torch.zeros(eval_iters)\n",
        "      for k in range(eval_iters):\n",
        "          X, Y = get_batch(split)\n",
        "          logits, loss = model(X, Y)\n",
        "          losses[k] = loss.item()\n",
        "      out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLT55sb_dXvP"
      },
      "source": [
        "# Define Model\n",
        "\n",
        "Self-attention head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvulWO6gaIgU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1)\n",
        "\n",
        "\"\"\"\n",
        "Single attention head\n",
        "\n",
        "Each index gets its corresponding key, query, and value matices by passing through the dense layer\n",
        "Query is matmuled with key (transposed so that the rows of querys are multiplied with the rows of keys that become columns)\n",
        "This is rescaled by the sqrt of embedding dims to avoid sharpening and keeping unit gaussian distribution\n",
        "\n",
        "Because this is decoder, use masked fill to prevent the model from seeing the future elements that its meant to generate\n",
        "\n",
        "rescaled key query product is multiplied by values to get the values that match\n",
        "\"\"\"\n",
        "class Head(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # (B, T, T)\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "    # perform the weighted aggregation of the values\n",
        "    v = self.value(x)\n",
        "    out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, n_head, head_size):\n",
        "    super().__init__()\n",
        "    # concatenate n_head heads\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # concatenate all the outputs of passing x through every head\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.proj(out)\n",
        "    return out\n",
        "\n",
        "\"\"\"\n",
        "FeedForward\n",
        "\n",
        "Linear transformation and ReLU followed by projection to n_embd dims\n",
        "Dropout to avoid overfitting for complex model\n",
        "\"\"\"\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "      super().__init__();\n",
        "      self.net = nn.Sequential(\n",
        "          nn.Linear(n_embd, 4 * n_embd), #grow channels for the residual block\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(4 * n_embd, n_embd), # projection layer, linear transformation no non-linear activation\n",
        "          nn.Dropout(dropout),\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\"\"\"\n",
        "Encoder Block\n",
        "\n",
        "These are stacked sequentially to form the full model\n",
        "Composed of self-attention, feedforwarad, and residual connections around both of the previous that get layernorm-ed and added\n",
        "\"\"\"\n",
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head;\n",
        "    self.sa_heads = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd) # layer norms to normalize across the time dimension (across the tokens within each batch)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # residual connections skipping over self attention and ffwd\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "\n",
        "    # every single index will retrieve an embedding corresponding to it\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size) # embedding dims -> vocab size logits\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    # map idx to embedding\n",
        "    tok_emb = self.token_embedding_table(idx) # (B,T,C), C is the embedding dimensions\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C) giving each token position from 0 - T in order\n",
        "    x = tok_emb + pos_emb # (B, T, C)\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x) # (B,T,C)\n",
        "    logits = self.lm_head(x) # put through decoder (B,T,vocab_size)\n",
        "\n",
        "    # targets optional in the cases that we want to generate new content\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      # Pytorch cross entropy expects flattened B*T so we do the same, note that -1 can be used to fill in remaining dim\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop idx to the last block_size tokens\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx_cond)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9pc09eLsB5b"
      },
      "source": [
        "# Initialize Model and Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoQDKXgzpgJ2",
        "outputId": "a73abb6e-27af-430b-c67c-e97de6666bba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 4.5064, val loss 4.5199\n",
            "step 10: train loss 2.9636, val loss 2.9959\n",
            "step 20: train loss 2.7465, val loss 2.7748\n",
            "step 30: train loss 2.6402, val loss 2.6641\n",
            "step 40: train loss 2.5837, val loss 2.6023\n",
            "step 50: train loss 2.5508, val loss 2.5736\n",
            "step 60: train loss 2.5254, val loss 2.5522\n",
            "step 70: train loss 2.5099, val loss 2.5397\n",
            "step 80: train loss 2.4982, val loss 2.5220\n",
            "step 90: train loss 2.4910, val loss 2.5211\n",
            "\n",
            "Mu thith him h tes hor theredowo ysod ombch aktre mise dageanth trh,\n",
            "\n",
            "\n",
            "THhis mpr hobloanimblore tes ous winddraisak'shemben ore\n",
            "\n",
            "Cl t heath m-hsean ieth' wis pro thawore ME: lse:\n",
            "\n",
            "Harshoosend, y e d, nor g sheque! ollt fs. n s memhe an fine ind be\n",
            "I'ly. ie\n",
            "Hed, witomar twe s te thond, wcoom odswhe garccere k.\n",
            "FAnan thir woman's at wapolowin,\n",
            "Thenthesis w t mea kld methe ven 'I f eieve n arand hinongissomy th h whathe ter me itl gorsthio her p ower os.\n",
            "Me s we d angs d s w'd d he, tobled:\n",
            "\n",
            "Hifid \n"
          ]
        }
      ],
      "source": [
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "g_l8tQr1JTQA",
        "qLqKo5JprrLl"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}